# Handwritten Digit Generation with GANs

This repository explores the generation of handwritten digits (MNIST dataset) using Generative Adversarial Networks (GANs). It includes implementations of different GAN architectures.

## About GANs

Generative Adversarial Networks (GANs) are a powerful class of deep learning models introduced by Ian Goodfellow et al. in 2014. They consist of two neural networks competing against each other:

1.  **Generator (G):** Tries to generate data (in this case, images of digits) that looks realistic, starting from random noise.
2.  **Discriminator (D):** Tries to distinguish between real data (from the actual dataset) and fake data generated by the Generator.

Through this adversarial process, the Generator learns to produce increasingly realistic data, while the Discriminator becomes better at spotting fakes.


## Deep Convolutional GAN (DC-GAN)

*(Will be detailed later.)*

---

## Conditional GAN (cGAN)

Conditional Generative Adversarial Network (cGAN) for generating specific handwritten digits (0-9) based on the MNIST dataset. Unlike a standard GAN, a cGAN learns to generate data conditioned on some additional information, typically class labels.

**Kaggle Notebook:** Implementation can be found in this Kaggle notebook: [Conditional GAN on Kaggle](https://www.kaggle.com/code/mohamedmohiey/conditional-gan)

### Concept

The core idea is to provide both the Generator and the Discriminator with the class label (the digit 0-9) as an additional input.

*   **Generator:** Receives random noise *and* a desired digit label. It must learn to generate an image that corresponds to that specific digit.
*   **Discriminator:** Receives an image (either real or fake) *and* its corresponding digit label. It must learn to determine if the image is a realistic example *of that specific digit*.


### Implementation Details (`Conditional-GAN.py`)

The implementation uses PyTorch and follows common practices for training cGANs on image data.

**Models:**

   *   **`Generator(nn.Module)`:**
        *   **Input:** Takes a concatenated tensor of random noise (from `get_noise`, shape `(B, Z, 1, 1)`) and one-hot encoded labels (shape `(B, C, 1, 1)`), resulting in an input shape of `(B, Z+C, 1, 1)`. `Z` is the noise dimension (`z_dim`), `C` is the number of classes (`n_classes`).
        *   **Architecture:** Uses a series of `ConvTranspose2d` (transposed convolution or deconvolution) layers with `BatchNorm2d` and `ReLU` activation functions to upsample the input noise+label vector into an image.
        *   **Output:** Produces an image of shape `(B, 1, 28, 28)` (matching MNIST). The final layer uses a `Tanh` activation function, mapping pixel values to the range `[-1, 1]`. This pairs well with the data normalization (`Normalize((0.5,), (0.5,))`) used for the real images.

   *   **`Discriminator(nn.Module)`:**
        *   **Input:** Takes a concatenated tensor of an image (real or fake, shape `(B, 1, 28, 28)`) and a one-hot label *map* (shape `(B, C, 28, 28)`), resulting in an input shape of `(B, 1+C, 28, 28)`.
        *   **Architecture:** Uses a series of `Conv2d` layers with `BatchNorm2d` and `LeakyReLU` activation functions to downsample the input image+label map.
        *   **Output:** Produces a single scalar logit value per input sample (shape `(B, 1)`), representing the probability that the input image is real *given the label*.

**Conditioning Mechanism:**

   *   **Label Preparation (`get_one_hot_labels`):** This crucial utility function takes integer labels (0-9) and converts them into two one-hot encoded formats suitable for the networks:
        1.  **Vector (`one_hot_labels_vec`):** Shape `(B, C, 1, 1)`. Used for concatenating with the noise vector for the Generator input.
        2.  **Map (`one_hot_labels_map`):** Shape `(B, C, H, W)`. Created by repeating the vector across spatial dimensions (H, W = 28, 28). Used for concatenating with the image tensor for the Discriminator input.
   *   **Concatenation (`concat_vectors`):** A simple helper function to concatenate tensors along the channel dimension (`dim=1`).

**Training Loop:**

   *   **Data:** MNIST dataset is loaded, normalized to `[-1, 1]`, and fed through a `DataLoader`.
   *   **Optimizers:** Adam optimizer is used for both Generator and Discriminator.
   *   **Loss Function:** `nn.BCEWithLogitsLoss` is used. This combines a Sigmoid layer and Binary Cross Entropy loss in one class.
   *   **Discriminator Training:**
        *   Takes real images + real label maps, and fake images (detached from Generator's graph) + corresponding label maps.
        *   Calculates loss for real images (target label `1`, potentially smoothed using `real_label_smoothing = 0.9` for stability) and fake images (target label `0`).
        *   Updates Discriminator weights.
   *   **Generator Training:**
        *   Generates fake images using noise + labels.
        *   Passes these fake images + label maps through the Discriminator.
        *   Calculates loss based on the Discriminator's output, using a target label of `1` (the Generator wants the Discriminator to classify its output as real).
        *   Updates Generator weights (gradients flow back through the Discriminator into the Generator).
   *   **Visualization:** Periodically (`display_step`), generated images for random labels are shown using `show_tensor_images`. After training, `visualize_all_digits` generates and displays examples for *each* digit class (0-9).

**Weights Initialization:** `weights_init` function applies a Normal distribution initialization to Convolutional and BatchNorm layers.

### Output Example

Below is an example grid showing generated digits (0-9) produced by the trained Conditional GAN after 24 epochs. Each row corresponds to a specific digit requested from the generator.

![Generated Digits from Conditional GAN](outputs/cgan_final_grid.png)
*(Image generated by `visualize_all_digits` after training)*